{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ae6874-7585-4a0b-848c-965f639def41",
   "metadata": {},
   "source": [
    "# Parsing Data\n",
    "In this notebook, let's explore how to leverage generative AI to build and consume a knowledge graph in Neo4j.\n",
    "\n",
    "This notebook parses Form-13 data from SEC EDGAR. This is partially structured data, a mix of text and XML.  Instead of spending our time writing a bespoke parser to extract data from these files and load into Neo4j, we can prompt a Large Language Model (LLM) to do this for us automatically.  We will then also use the LLM to generate Cypher statements to load the extracted data into a Neo4j graph.\n",
    "\n",
    "## Setup\n",
    "First, let's install the libraries we're going to need for this lab and the following notebook dependent labs.  We'll also want to reboot the kernel once done.  To do that, go to the \"Kernel\" menu and click \"Restart Kernel and Clear All Outputs.\"  That will get rid of everything the install statements printed, leaving us with a cleaner notebook to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aad4e36-7048-4bbf-a71a-a5d1b24d0ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphdatascience in /home/jupyter/.local/lib/python3.10/site-packages (1.10)\n",
      "Requirement already satisfied: multimethod<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (1.11.2)\n",
      "Requirement already satisfied: neo4j<6.0,>=4.4.2 in /home/jupyter/.local/lib/python3.10/site-packages (from graphdatascience) (5.20.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (2.0.3)\n",
      "Requirement already satisfied: pyarrow<16.0,>=11.0 in /home/jupyter/.local/lib/python3.10/site-packages (from graphdatascience) (15.0.2)\n",
      "Requirement already satisfied: textdistance<5.0,>=4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from graphdatascience) (4.6.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (4.11.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from graphdatascience) (2.31.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from neo4j<6.0,>=4.4.2->graphdatascience) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2.9.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->graphdatascience) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->graphdatascience) (1.25.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->graphdatascience) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->graphdatascience) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests->graphdatascience) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->graphdatascience) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->graphdatascience) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pydantic==1.10.11 in /home/jupyter/.local/lib/python3.10/site-packages (1.10.11)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==1.10.11) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain==0.1.20 in /home/jupyter/.local/lib/python3.10/site-packages (0.1.20)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain==0.1.20) (0.6.6)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.38 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain==0.1.20) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain==0.1.20) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain==0.1.20) (0.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain==0.1.20) (0.1.63)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain==0.1.20) (1.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain==0.1.20) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.20) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.20) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain==0.1.20) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.20) (3.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.1.20) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.20) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.20) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.20) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain==0.1.20) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.20) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain==0.1.20) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.20) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gradio in /home/jupyter/.local/lib/python3.10/site-packages (4.31.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (5.3.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.111.0)\n",
      "Requirement already satisfied: ffmpy in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.16.4 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (0.16.4)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (0.23.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.3)\n",
      "Requirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.25.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.3)\n",
      "Requirement already satisfied: packaging in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.3.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "Requirement already satisfied: pydub in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (0.4.5)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: urllib3~=2.0 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio) (2.2.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.29.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.16.4->gradio) (2024.3.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /home/jupyter/.local/lib/python3.10/site-packages (from gradio-client==0.16.4->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.22.0)\n",
      "Requirement already satisfied: toolz in /home/jupyter/.local/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.3.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.0.3)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (2.1.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi->gradio) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.11\n",
      "    Uninstalling pydantic-1.10.11:\n",
      "      Successfully uninstalled pydantic-1.10.11\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataproc-jupyter-plugin 0.1.74 requires pydantic~=1.10.0, but you have pydantic 2.7.1 which is incompatible.\n",
      "ydata-profiling 4.6.0 requires pydantic<2,>=1.8.1, but you have pydantic 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-2.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: IProgress in /home/jupyter/.local/lib/python3.10/site-packages (0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from IProgress) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-community in /home/jupyter/.local/lib/python3.10/site-packages (0.0.38)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-community) (0.6.6)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-community) (0.1.52)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-community) (0.1.63)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (2.7.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.52->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.52->langchain-community) (2.18.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-google-vertexai in /home/jupyter/.local/lib/python3.10/site-packages (1.0.4)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.47.0 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (1.51.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.14.0 in /opt/conda/lib/python3.10/site-packages (from langchain-google-vertexai) (2.14.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.42 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-google-vertexai) (0.1.52)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (23.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (3.22.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (2.0.4)\n",
      "Requirement already satisfied: pydantic<3 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (2.7.1)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (2.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (2.31.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (1.5.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.42->langchain-google-vertexai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.42->langchain-google-vertexai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /home/jupyter/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.42->langchain-google-vertexai) (0.1.63)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.42->langchain-google-vertexai) (8.3.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (0.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.42->langchain-google-vertexai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.42->langchain-google-vertexai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /home/jupyter/.local/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jupyter/.local/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.14.0->langchain-google-vertexai) (2024.2.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (1.25.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.47.0->langchain-google-vertexai) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install --user graphdatascience\n",
    "%pip install --user \"pydantic==1.10.11\"\n",
    "%pip install --user \"langchain==0.1.20\"\n",
    "%pip install --user gradio\n",
    "%pip install --user IProgress\n",
    "%pip install --user tqdm\n",
    "%pip install --user langchain-community\n",
    "%pip install --user langchain-google-vertexai\n",
    "\n",
    "# Restart the kernel after installing libraries\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7c539-68b5-4126-990e-60c85b84fafa",
   "metadata": {},
   "source": [
    "Now restart the kernel. That will allow the Python evironment to import the new packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1a385-8d3a-4c69-9817-ed14ba8e6511",
   "metadata": {},
   "source": [
    "## Prompt Definition\n",
    "We will extract knowledge adhering to the same schema we used previously.  To teach the LLM about the schema, we will use a series of prompts.  Each prompt is focused on only one task, extracting a specific entity:\n",
    "\n",
    "1. Manager Information\n",
    "2. Filing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ffeab081-6bc7-41a1-92a6-1f02b8adcf8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "station_info_tpl = \"\"\"The input file below is a CSV file. The first line represents the column names. Extract the following entities and put them into JSON format. Do not miss any of this information.\n",
    "* \"station_name\" - This name appears in the \"Station_Name\" column of the CSV file. \n",
    "* \"latitude\" - This value appears in the \"Latitude\" column of the source file.\n",
    "* \"longitude\" - This value appears in the \"Longitude\" column of the source file.\n",
    "* \"postcode\" - This value appears in the \"Postcode\" column of the source file. \n",
    "* \"zone\" - This value appears in the \"Zone\" column of the source file. \n",
    "* Only return me the JSON enclosed by 3 backticks. No other text in the response\n",
    "\n",
    "Text:\n",
    "$ctext\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2483efd5-fedf-49b0-9f01-534bd7390968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tube_line_info_tpl = \"\"\"The input file below is a CSV file. The first line represents the column names. Please extract the below variables into json then combine into a list enclosed by 3 backticks. Please use the quoted names below while doing this\n",
    "* \"tube_line\" - This appears in the column \"Tube_Line\" of the CSV file. \n",
    "* \"from_station\" - This is the name of the originating station. It appears in the \"From_Station\" column.\n",
    "* \"to_station\" - This is the name of the originating station. It appears in the \"To_Station\" column.\n",
    "\n",
    "Output format:\n",
    "The output should be a valid JSON list enclosed by 3 backticks\n",
    "\n",
    "Text:\n",
    "$ctext\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481717bb-6db2-4465-8667-b0c07da381d5",
   "metadata": {},
   "source": [
    "## Functions for Using LLMs\n",
    "Let's create some helper function to talk to the LLM with our prompt and text input. \n",
    "\n",
    "The [Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models) describes the available foundation models.  We will use the text-bison base model. In some cases, there may be a need to fine-tune LLM models for KG creation. [Vertex AI provides an elegant way to fine-tune](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-models) where the updated weights/model stay within your tenant and the base model is frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e42db05-86be-4171-9ef3-538af376057e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "# Wrapper for calling language model\n",
    "def run_text_model(\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_decode_steps: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    prompt: str,\n",
    "    tuned_model_name: str = None,\n",
    "    ) :\n",
    "    \"\"\"Text Completion Use a Large Language Model.\"\"\"\n",
    "    if tuned_model_name is None:\n",
    "        model = TextGenerationModel.from_pretrained(model_name)\n",
    "    else:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        prompt,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_decode_steps,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ca2aab-87a9-4628-9f17-6a8fd7d08153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrapper for entity extraction and parsing\n",
    "def extract_entities_relationships(prompt, tuned_model_name=None):\n",
    "    try:\n",
    "        res = run_text_model(\"text-bison@001\", 0, 1024, 0.8, 1, prompt, tuned_model_name)\n",
    "        return res\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726ef3e9-e66f-45ed-be59-d386ce1db4a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# splitting function for chunking up filing information to avoid hitting LLM token limits\n",
    "def split_filing_info(s, chunk_size=5):\n",
    "    pattern = '(</(\\w+:)?infoTable>)'\n",
    "    splitter = re.findall(pattern, s)[0][0]\n",
    "    _parts = s.split(splitter)\n",
    "    if len(_parts) > chunk_size:\n",
    "        chunks_of_list = np.array_split(_parts, len(_parts)/chunk_size) # max 5 filings per part\n",
    "        chunks_of_str = map(lambda x: splitter.join(x)+splitter, chunks_of_list)\n",
    "        l = list(chunks_of_str)\n",
    "        if len(l) > 0:\n",
    "            l[len(l)-1] = re.sub(f'{splitter}$', '', l[len(l)-1])\n",
    "        return l\n",
    "    else:\n",
    "        return [s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb9dee8-c5dc-42ad-b05d-0f0d7e11d551",
   "metadata": {},
   "source": [
    "## Test Example for Parsing\n",
    "Let's start with one Form 13 file to see how we can parse it with Generative AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57cc2dfa-4947-4afc-8b2a-6cd16338029a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket('neo4j-datasets')\n",
    "blob = bucket.blob('dataflow-london-transport/bigquery-to-neo4j/source-data/London_stations.csv')\n",
    "\n",
    "inp_text = blob.download_as_string().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d82c1ec-d49d-4444-b4fe-bc0be2777308",
   "metadata": {},
   "source": [
    "We can take a look at the file.  Note that it is an oddball mix of XML, delimeted and fixed spacing formatting that no standard parser could make sense of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33cda3eb-25c6-47cf-8d11-e1981766aeae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,Station_Name,OS X,OS Y,Latitude,Longitude,Zone,Postcode,Zone_original\n",
      "1,Abbey Road,539081,183352,51.53195199547290,0.003723371,3,E15 3NB,3\n",
      "2,Abbey Wood,547297,179002,51.49078429543710,0.12027197064065100,4,SE2 9RH,4\n",
      "3,Acton Central,520613,180299,51.508757351253400,-0.263430195,2,W3 6BH,2\n",
      "4,Acton Main Line,520296,181196,51.516886495162900,-0.267689948,3,W3 9EH,3\n",
      "5,Acton Town,519457,179639,51.50307098636340,-0.280302697,3,W3 8HN,3\n",
      "6,Addington Village,537082,163744,51.35623853949500,-0.0326651,3,CR0 5AR,\"3,4,5,6\"\n",
      "7,Addiscombe,534190,166290,51.37980817216770,-0.073213283,3,CR0 7AA,\"3,4,5,6\"\n",
      "8,Albany Park,547903,172902,51.435815816166300,0.1264446387204620,5,DA5 3HP,5\n",
      "9,Aldgate,533629,181246,51.51434226,-0.075626912,1,EC3N 1AH,1\n",
      "10,Aldgate East,533809,181333,51.515081505698700,-0.073001462,1,E1 7PT,1\n",
      "11,Alexandra Palace,530300,190498,51.59826262860600,-0.120148971,3,N22 7ST,3\n",
      "12,All Saints,538012,180933,51.51047690641240,-0.012624885,2,E14 0EH,2\n",
      "13,Alperton,518025,183849,51.54120878643840,-0.299515753,4,HA0 4LL,4\n",
      "14,Amersham,496454,198181,51.674128328342700,-0.606513768,9,HP6 5AZ,9\n",
      "15,Ampere Way,530674,166476,51.38229921988810,-0.123636798,3,CR0 3JX,\"3,4,5,6\"\n",
      "16,Anerley,534656,169942,51.41251661375110,-0.0651371,4,SE20 8PY,4\n",
      "17,Angel,531497,183263,51.53296784539840,-0.105581448,1,N1 8XB,1\n",
      "18,Angel Road,535204,192202,51.612420242475800,-0.048732919,4,N18 3AY,4\n",
      "19,Archway,529356,186827,51.56549031545970,-0.135121576,2,N19 5RQ,\"2,3\"\n",
      "20,Arena,535192,167620,51.39\n"
     ]
    }
   ],
   "source": [
    "print(inp_text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41717f-0c6e-4642-b6fd-ca327ce474cf",
   "metadata": {},
   "source": [
    "We can split data into manager and filing info pieces using `<XML>` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f23c86-3274-40fd-9c97-70337d8dd535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = inp_text.split('<XML>')\n",
    "manager_info = contents[1].split('</XML>')[0].strip()\n",
    "filing_info = contents[2].split('</XML>')[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21286ffa-2ff2-40cd-8eee-8b4626f32c1f",
   "metadata": {},
   "source": [
    "## Parsing Manager Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d7f7b-5b24-4d05-b0b6-a3d8a85dcad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "\n",
    "vertexai.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed35d7c5-ef70-419b-b9c3-69c34003a75e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input file below is a CSV file. The first line represents the column names. Extract the following entities and put them into JSON format. Do not miss any of this information.\n",
      "* \"station_name\" - This name appears in the \"Station_Name\" column of the CSV file. \n",
      "* \"latitude\" - This value appears in the \"Latitude\" column of the source file.\n",
      "* \"longitude\" - This value appears in the \"Longitude\" column of the source file.\n",
      "* \"postcode\" - This value appears in the \"Postcode\" column of the source file. \n",
      "* \"zone\" - This value appears in the \"Zone\" column of the source file. \n",
      "* Only return me the JSON enclosed by 3 backticks. No other text in the response\n",
      "\n",
      "Text:\n",
      "ID,Station_Name,OS X,OS Y,Latitude,Longitude,Zone,Postcode,Zone_original\n",
      "1,Abbey Road,539081,183352,51.53195199547290,0.003723371,3,E15 3NB,3\n",
      "2,Abbey Wood,547297,179002,51.49078429543710,0.12027197064065100,4,SE2 9RH,4\n",
      "3,Acton Central,520613,180299,51.508757351253400,-0.263430195,2,W3 6BH,2\n",
      "4,Acton Main Line,520296,181196,51.516886495162900,-0.267689948,3,W3 9EH,3\n",
      "5,Acton Town,519457,179639,51.50307098636340,-0.280302697,3,W3 8HN,3\n",
      "6,Addington Village,537082,163744,51.35623853949500,-0.0326651,3,CR0 5AR,\"3,4,5,6\"\n",
      "7,Addiscombe,534190,166290,51.37980817216770,-0.073213283,3,CR0 7AA,\"3,4,5,6\"\n",
      "8,Albany Park,547903,172902,51.435815816166300,0.1264446387204620,5,DA5 3HP,5\n",
      "9,Aldgate,533629,181246,51.51434226,-0.075626912,1,EC3N 1AH,1\n",
      "10,Aldgate East,533809,181333,51.515081505698700,-0.073001462,1,E1 7PT,1\n",
      "11,Alexandra Palace,530300,190498,51.59826262860600,-0.120148971,3,N22 7ST,3\n",
      "12,All Saints,538012,180933,51.51047690641240,-0.012624885,2,E14 0EH,2\n",
      "13,Alperton,518025,183849,51.54120878643840,-0.299515753,4,HA0 4LL,4\n",
      "14,Amersham,496454,198181,51.674128328342700,-0.606513768,9,HP6 5AZ,9\n",
      "15,Ampere Way,530674,166476,51.38229921988810,-0.123636798,3,CR0 3JX,\"3,4,5,6\"\n",
      "16,Anerley,534656,169942,51.41251661375110,-0.0651371,4,SE20 8PY,4\n",
      "17,Angel,531497,183263,51.53296784539840,-0.105581448,1,N1 8XB,1\n",
      "18,Angel Road,535204,192202,51.612420242475800,-0.048732919,4,N18 3AY,4\n",
      "19,Archway,529356,186827,51.56549031545970,-0.135121576,2,N19 5RQ,\"2,3\"\n",
      "20,Arena,535192,167620,51.39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from string import Template\n",
    "\n",
    "prompt = Template(station_info_tpl).substitute(ctext=inp_text[:1500])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b13100f-de4b-49f9-89f6-58e01c19727e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'station_name': 'Abbey Road',\n",
       " 'latitude': '51.53195199547290',\n",
       " 'longitude': '0.003723371',\n",
       " 'postcode': 'E15 3NB',\n",
       " 'zone': '3'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Use LLM to parse out manager information\n",
    "station_data = json.loads(extract_entities_relationships(prompt).split('```')[1].strip('json'))\n",
    "station_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775186c0-d2bc-4711-88c4-0e61488ad2bd",
   "metadata": {},
   "source": [
    "## Parse Filing Information\n",
    "We will parse filing info in a similar manner to manager information. Because the filings include a list of many entries however, we will want to split the input into chunks so as not to exceed input or output token limits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aa1690a-012b-4056-8fcc-6e04e4bb26f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ID,Station_Name,OS X,OS Y,Latitude,Longitude,Zone,Postcode,Zone_original\\r\\n1,Abbey Road,539081,183352,51.53195199547290,0.003723371,3,E15 3NB,3\\r\\n2,Abbey Wood,547297,179002,51.49078429543710,0.12027197064065100,4,SE2 9RH,4\\r\\n3,Acton Central,520613,180299,51.508757351253400,-0.263430195,2,W3 6BH,2\\r\\n4,Acton Main Line,520296,181196,51.516886495162900,-0.267689948,3,W3 9EH,3\\r\\n5,Acton Town,519457,179639,51.50307098636340,-0.280302697,3,W3 8HN,3\\r\\n6,Addington Village,537082,163744,51.35623853949500,-0.0326651,3,CR0 5AR,\"3,4,5,6\"\\r\\n7,Addiscombe,534190,166290,51.37980817216770,-0.073213283,3,CR0 7AA,\"3,4,5,6\"\\r\\n8,Albany Park,547903,172902,51.435815816166300,0.1264446387204620,5,DA5 3HP,5\\r\\n9,Aldgate,533629,181246,51.51434226,-0.075626912,1,EC3N 1AH,1\\r\\n10,Aldgate East,533809,181333,51.515081505698700,-0.073001462,1,E1 7PT,1\\r\\n11,Alexandra Palace,530300,190498,51.59826262860600,-0.120148971,3,N22 7ST,3\\r\\n12,All Saints,538012,180933,51.51047690641240,-0.012624885,2,E14 0EH,2\\r\\n13,Alperton,518025,183849,51.54120878643840,-0.299515753,4,HA0 4LL,4\\r\\n14,Amersham,496454,198181,51.674128328342700,-0.606513768,9,HP6 5AZ,9\\r\\n15,Ampere Way,530674,166476,51.38229921988810,-0.123636798,3,CR0 3JX,\"3,4,5,6\"\\r\\n16,Anerley,534656,169942,51.41251661375110,-0.0651371,4,SE20 8PY,4\\r\\n17,Angel,531497,183263,51.53296784539840,-0.105581448,1,N1 8XB,1\\r\\n18,Angel Road,535204,192202,51.612420242475800,-0.048732919,4,N18 3AY,4\\r\\n19,Archway,529356,186827,51.56549031545970,-0.135121576,2,N19 5RQ,\"2,3\"\\r\\n20,Arena,535192,167620,51.39'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = bucket.blob('dataflow-london-transport/bigquery-to-neo4j/source-data/London_stations.csv')\n",
    "\n",
    "inp_text2 = blob.download_as_string().decode()\n",
    "inp_text2[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "125ed4c0-22b9-4fc8-83f0-a1ccc5eaa112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "[\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Road\",\n",
      "    \"to_station\": \"Edgware Road\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Road\",\n",
      "    \"to_station\": \"Kilburn Park\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Road\",\n",
      "    \"to_station\": \"Queen's Park\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Road\",\n",
      "    \"to_station\": \"Willesden Junction\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Wood\",\n",
      "    \"to_station\": \"Canning Town\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Wood\",\n",
      "    \"to_station\": \"Dagenham East\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Wood\",\n",
      "    \"to_station\": \"Elverson Road\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Wood\",\n",
      "    \"to_station\": \"Greenwich\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Wood\",\n",
      "    \"to_station\": \"North Greenwich\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Wood\",\n",
      "    \"to_station\": \"Plaistow\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Abbey Wood\",\n",
      "    \"to_station\": \"Stratford\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Central\",\n",
      "    \"to_station\": \"Ealing Broadway\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Central\",\n",
      "    \"to_station\": \"Ealing Common\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Central\",\n",
      "    \"to_station\": \"Ealing Park\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Central\",\n",
      "    \"to_station\": \"North Ealing\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Central\",\n",
      "    \"to_station\": \"Park Royal\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Central\",\n",
      "    \"to_station\": \"West Acton\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Main Line\",\n",
      "    \"to_station\": \"Ealing Broadway\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Main Line\",\n",
      "    \"to_station\": \"Ealing Common\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Main Line\",\n",
      "    \"to_station\": \"Ealing Park\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Main Line\",\n",
      "    \"to_station\": \"North Ealing\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Main Line\",\n",
      "    \"to_station\": \"Park Royal\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Main Line\",\n",
      "    \"to_station\": \"West Acton\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Town\",\n",
      "    \"to_station\": \"Ealing Broadway\"\n",
      "  },\n",
      "  {\n",
      "    \"tube_line\": \"Bakerloo\",\n",
      "    \"from_station\": \"Acton Town\",\n",
      "    \"to_station\n"
     ]
    }
   ],
   "source": [
    "prompt = Template(tube_line_info_tpl).substitute(ctext=inp_text2[:1500])\n",
    "response = extract_entities_relationships(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2560b-2e6c-4b96-abc3-7c96196d0eb5",
   "metadata": {},
   "source": [
    "## Test Example\n",
    "\n",
    "Let's walk through the steps to do this with just the 1 form above first, then we can move on to parsing and ingesting multiple form13s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a67de-b18f-4c65-a529-b890086663b8",
   "metadata": {},
   "source": [
    "To start we can run the LLM parsing over all the filing info from the form and then combine the resulting JSON into a list conducive for Neo4j loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827621eb-2f7d-4ef4-9f00-cddbc5e2cfd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filings_list = []\n",
    "import time\n",
    "for filing_info_chunk in filing_info_chunks:\n",
    "    prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunk)\n",
    "    response = extract_entities_relationships(prompt)\n",
    "    # time.sleep(2) #uncomment this line if you face any rate limit error\n",
    "    if '```' in response:\n",
    "        response = response.split('```')[1].strip('json')\n",
    "    filings_list.extend(json.loads(response))\n",
    "\n",
    "for item in filings_list:\n",
    "    item['managerName'] = manager_data['managerName']\n",
    "    item['reportCalendarOrQuarter'] = manager_data['reportCalendarOrQuarter']\n",
    "filings_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6b661-7d08-42fa-9dda-6ea9e8dfcffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(filings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b83ca6-c44b-4f43-9dcd-c88cf525a978",
   "metadata": {},
   "source": [
    "## Establish Neo4j Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fffd14-e6bb-4843-90e9-d1d4e7cef3cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# username is neo4j by default\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "\n",
    "# You will need to change these to match your credentials\n",
    "NEO4J_URI = 'neo4j+s://f037e189.databases.neo4j.io'\n",
    "NEO4J_PASSWORD = 'Ka9u8KfClrPlLj79a27-E2caAi14VO_BwM4b2W1fvos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e2ef1-f5bd-4561-87e7-a014b8af2e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "gds = GraphDataScience(\n",
    "    NEO4J_URI,\n",
    "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
    "    aura_ds=True\n",
    ")\n",
    "gds.set_database('neo4j')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f6adf6-a299-47ca-a965-6b6cdec67f77",
   "metadata": {},
   "source": [
    "Before loading, we should create node key constraints for nodes.  This acts as a unique id and an index and is necessary for fast, efficient queries.  In general, if you notice ingestion is super slow (and getting slower) with Neo4j, double-check that you created indexes.  For this small sample, it won't matter, but it will undoubtedly impact as we ingest more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3084e75-da30-42c1-8ca4-10cf55e7c3d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gds.run_cypher('CREATE CONSTRAINT unique_manager IF NOT EXISTS FOR (n:Manager) REQUIRE (n.managerName) IS NODE KEY')\n",
    "gds.run_cypher('CREATE CONSTRAINT unique_company_id IF NOT EXISTS FOR (n:Company) REQUIRE (n.cusip) IS NODE KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814834e0-54f6-4e6f-9a6e-67a526cf3bf6",
   "metadata": {},
   "source": [
    "To merge the data, we can use parameterized Cypher queries.  Basically, we will send filings in batches (in this sample case, just one batch) for each node and relationship type and insert them as parameters in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb25f22-39a0-4930-bf05-8adaa45c2f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge company nodes\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MERGE (c:Company {cusip: record.cusip})\n",
    "SET c.companyName = record.companyName\n",
    "RETURN count(c) AS company_node_merge_count\n",
    "''', params={'records':filings_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635b9393-a2b6-4da6-94a4-cca6bcf515bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge manager node\n",
    "gds.run_cypher('''\n",
    "MERGE (m:Manager {managerName: $name})\n",
    "RETURN count(m) AS manager_node_merge_count\n",
    "''', params={'name':manager_data['managerName']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5394e270-45c3-44c9-96bb-6ea210dbb28a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge owns Relationship\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MATCH (m:Manager {managerName: record.managerName})\n",
    "MATCH (c:Company {cusip: record.cusip})\n",
    "MERGE(m)-[r:OWNS]->(c)\n",
    "SET r.reportCalendarOrQuarter = record.reportCalendarOrQuarter,\n",
    "    r.value = record.value,\n",
    "    r.shares = record.shares\n",
    "RETURN count(r) AS owns_relationship_merge_count\n",
    "''', params={'records':filings_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7ebde6-7e39-40fc-8457-4d39bb45efd1",
   "metadata": {},
   "source": [
    "## Ingest Multiple Form 13 Files\n",
    "We will make a pipeline using the methods above.  In this case we will take a two-step approach, first parse all the data, then chunk that data and ingest into Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0900b-0312-4213-9a3d-13e512226f93",
   "metadata": {},
   "source": [
    "For purposes of this lab we will just use a few form13 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1d593-dd40-4f08-ac0d-9c8e9ff201d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### If you have time to parse more files, you can uncomment these lines.\n",
    "sample_file_names = [\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1844571_0001844571-22-000001.txt',\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-03_archives_edgar_data_1875995_0001875995-22-000004.txt',\n",
    "   'hands-on-lab/form13-raw/raw_2022-01-06_archives_edgar_data_1495703_0001495703-22-000002.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c27a4-d6bf-4e12-af6e-478e20fe5ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function for getting filing info\n",
    "def get_manager_and_filing_info(raw_txt):\n",
    "    contents = raw_txt.split('<XML>')\n",
    "    manager_info = contents[1].split('</XML>')[0].strip()\n",
    "    filing_info = contents[2].split('</XML>')[0].strip()\n",
    "    \n",
    "    return manager_info, filing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46cc84-53b5-4088-a5aa-578f7eb5d0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(f'=== Parsing {len(sample_file_names)} Form 13 Files ===')\n",
    "\n",
    "filings_list = []\n",
    "manager_list = []\n",
    "\n",
    "for file_name in sample_file_names:\n",
    "    \n",
    "    print(f'--- parsing {file_name} ---')\n",
    "    try:\n",
    "        # Get raw form13 file\n",
    "        print('getting file text from gcloud....')\n",
    "        blob = bucket.blob(file_name)\n",
    "        raw_text = blob.download_as_string().decode()\n",
    "\n",
    "        # Get raw manager and filing info from file\n",
    "        print('getting file contents...')\n",
    "        manager_info, filing_info = get_manager_and_filing_info(raw_text)\n",
    "\n",
    "        # Parse manager info into dict using LLM\n",
    "        print('Parsing submission and manager info...')\n",
    "        mng_prompt = Template(mgr_info_tpl).substitute(ctext=manager_info)\n",
    "        mng_response = extract_entities_relationships(mng_prompt)\n",
    "        manager_data = json.loads(mng_response.replace('```', ''))\n",
    "        manager_list.append({'managerName': manager_data['managerName']})\n",
    "\n",
    "        # Parse filing info into list of dicts using LLM\n",
    "        print('Parsing filing info...')\n",
    "        tmp_filing_list = []\n",
    "        for filing_info_chunk in split_filing_info(filing_info):\n",
    "            filing_prompt = Template(filing_info_tpl).substitute(ctext=filing_info_chunk)\n",
    "            filing_response = extract_entities_relationships(filing_prompt)\n",
    "            #time.sleep(3) #uncomment this line if you face any rate limit error\n",
    "            if '```' in filing_response:\n",
    "                filing_response = filing_response.split('```')[1].strip('json')\n",
    "            tmp_filing_list.extend(json.loads(filing_response))\n",
    "        for item in tmp_filing_list: #Add information from manager_info to enable OWNS relationship loading\n",
    "            item['managerName'] = manager_data['managerName']\n",
    "            item['reportCalendarOrQuarter'] = manager_data['reportCalendarOrQuarter']\n",
    "        filings_list.extend(tmp_filing_list)\n",
    "    except Exception as e:\n",
    "        print(filing_response)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b656917-8b98-4b94-a905-fcd4a4a5f8bd",
   "metadata": {},
   "source": [
    "Now we can merge the mananger nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9206bb-d619-4649-99cf-f5939461555a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge manager nodes\n",
    "gds.run_cypher('''\n",
    "UNWIND $records AS record\n",
    "MERGE (m:Manager {managerName: record.managerName})\n",
    "RETURN count(m) AS manager_node_merge_count\n",
    "''', params={'records':manager_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c27c4-4e48-4a08-8aba-26a3874e7f3e",
   "metadata": {},
   "source": [
    "For filings lets check ther length of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31f300-1c53-4795-ae1c-7228f240ddf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(filings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ede58-e686-42c8-90f7-fd8919a17f36",
   "metadata": {},
   "source": [
    "While we should not need chunking for this example, below is an example of how to chunk up a parameterized function for loading in case you need to scale up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb716e1-c71c-4992-b654-016f9496f08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As the dataset gets bigger we will want to chunk up the filings we send to Neo4j\n",
    "def chunks(xs, n=10_000):\n",
    "    n = max(1, n)\n",
    "    return [xs[i:i + n] for i in range(0, len(xs), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2216015-d8ca-475d-8240-a5717081c8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge company nodes\n",
    "for d in chunks(filings_list):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MERGE (c:Company {cusip: record.cusip})\n",
    "    SET c.companyName = record.companyName\n",
    "    RETURN count(c) AS company_node_merge_count\n",
    "    ''', params={'records':d})\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750215eb-ec16-4b42-909f-cf763b9955b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge owns Relationships\n",
    "for d in chunks(filings_list):\n",
    "    res = gds.run_cypher('''\n",
    "    UNWIND $records AS record\n",
    "    MATCH (m:Manager {managerName: record.managerName})\n",
    "    MATCH (c:Company {cusip: record.cusip})\n",
    "    MERGE(m)-[r:OWNS]->(c)\n",
    "    SET r.reportCalendarOrQuarter = record.reportCalendarOrQuarter,\n",
    "        r.value = record.value,\n",
    "        r.shares = record.shares\n",
    "    RETURN count(r) AS owns_relationship_merge_count\n",
    "    ''', params={'records':d})\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88924cd-b43a-42ca-b975-87f1362987fc",
   "metadata": {},
   "source": [
    "This type of workflow can be applied to other unstructured data to parse entities and relationships with language models and load them into a Neo4j knowledge graph. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cpu:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
